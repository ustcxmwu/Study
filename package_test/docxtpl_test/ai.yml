- question: 什么是强化学习？跟监督学习和非监督学习的区别是什么？
  answer: 强化学习侧重从交互中获取奖励或者惩罚来指导行动
- question: MDP性质，MDP组成
  answer: 无后效性，五元组（S,A,R,T, gamma)
- question: Model based 和 Model free的区别
  answer: 状态转移概率是否已知或者需要
- question: 强化学习的损失函数是什么？和深度学习的损失函数有何关系？
  answer: 强化学习的损失函数是使奖励和的期望最大；深度学习中的损失函数是使预测值和标签之间的差异最小化。
- question: 什么是 on-policy 和 off-policy，举例说明
  answer: 在线策略用于学习和用于采样的是同一个策略，离线策略中行动策略用来采样，目标策略是用来学习的。在线策略不学习最优动作而是学习一个接近最优动作同时又能继续探索的动作，离线策略直接学习最优动作。
- question: 简述蒙特卡罗估计值函数(MC)算法
  answer: MC算法是model-free的学习方法, 它从”经验“中学习价值函数和最优策略，”经验“是指多幕采样数据，MC通过平均样本的回报在大数定律的保证下进行策略估计，然后采用柔性策略进行MC控制。MC算法是深度采样更新，它没有使用自举法。
- question: 简述时间差分(TD)算法
  answer: TD算法和MC算法一样可以从和环境互动的经验中学习策略而不依赖环境的动态特性，TD和DP一样都采用的自举法，是采样更新。和MC不同的是TD算法采样深度没有那么深，它不是一个完全的采样，TD的策略评估是根据它直接得到的后继状态节点的单次样本转移来更新的，换言之它不需要等到一幕完全结束而是可以立刻进行学习。它采用后继状态的价值和沿途的收益进行更新，TD控制有Sarsa、期望Sarsa和Q学习。
- question: value-based和policy-based的区别是什么
  answer:
    - 处理的action space不同：value-based适合处理的action space低维离散的，policy-based适合处理连续的action space。
    - 针对action的价值输出不同：value-based计算出每个action的价值，policy-based一般情况下只给出较价值较高的actions。
    - 更新频率不同：value-based每个action执行都可以更新，policy-based 每个episode完成之后才能更新一次。
- question: DQN都有哪些变种？
  answer:
    - Double-DQN：将动作选择和价值估计分开，避免价值过高估计。
    - Dueling-DQN：将Q值分解为状态价值和优势函数，得到更多有用信息。
    - Prioritized Replay Buffer：将经验池中的经验按照优先级进行采样。
- question: 简述Double DQN原理？
  answer:
- question: 什么是偏差和方差
  answer: 偏差是真实值和预测值之间的偏离程度；方差是预测值得分散程度，即越分散，方差越大；
- question: 什么是预测问题
  answer:
    - 输入 MDP(S, A, R, P, gamma) 和策略 pi
    - 输出 最优状态值函数 V, 或者最优状态动作值函数 Q
- question: 什么是控制问题
  answer:
    - 输入 MDP(S, A, R, P, gamma)
    - 输出 最优状态值函数 V 或者最优状态动作值函数Q, 和最优策略 pi
- question: 什么是探索和利用 exploration and exploitation
  answer:
- question: DQN的三个tip是什么？
  answer: 1) Target Network 2) Epsilon Greedy Exploration 3) Experience Replay
- question: Experience Replay Buff 的优点
  answer: 1) 提升样本的使用效率，提升训练效率；2) 从Replay Buff采样道德数据具有多样性，降低了样本的相关性。
- question: DQN 和 Q-Learning有什么不同？
  answer: 1) DQN 将 Q-Learning 与深度学习结合，采用深度网络来近似动作价值函数，而 Q-Learning 则是采用表格存储；2) DQN 采用了经验回放的训练方法，从历史数据中随机采样，而 Q-Learning 直接采用下一个状态的数据进行学习
- question: rainbow 算法中试用的tip
  answer: 1) DQN 2) DDQN 3) Prioritized DDQN 4) Dueling DDQN 5) A3C 6) Distributed DQN 7) Noisy DQN
- question: 简述 Double DQN原理
  answer: DQN 由于总是选择当前值函数最大的动作值函数来更新当前的动作值函数，因此存在着过估计问题。为了解耦这两个过程，Double DQN 使用了两个值网络，一个玩过用来执行动作选择，然后用另一个值函数对一个的动作值更新当前网络。
- question: 演员-评论家算法（Actor-Critic Algorithm），演员和评论家指的是什么
  answer: 演员 Actor 指的是策略函数 pi_theta(a|s) 即学习一个策略来得到尽量搞得回报；评论家 Critic 是指值函数 V^pi(s)，对当前策略的值函数进行估计，即评论演员的好坏。
- question: 值迭代和策略迭代的区别?
  answer: 策略迭代。它有两个循环，一个是在策略估计的时候，为了求当前策略的值函数需要迭代很多次。 另外一个是外面的大循环，就是策略评估，策略提升这个循环。值迭代算法则是一步到位，直接估计 最优值函数，因此没有策略提升环节。 参考值迭代算法
- question: 求解马尔科夫决策过程都有哪些方法？有模型用什么方法？动态规划是怎么回事？
  answer: 方法有：动态规划，时间差分，蒙特卡洛。 有模型可以使用动态规划方法。 动态规划是指：将一个问题拆分成几个子问题，分别求解这些子问题，然后获得原问题的解。 贝尔曼方程中为了求解一个状态的值函数，利用了其他状态的值函数，就是这种思想.
- question: MC和TD分别是无偏估计吗？
  answer: MC是无偏的，TD是有偏的
- question: MC、TD谁的方差大，为什么？
  answer: MC的方差大，以为TD使用了自举，实现一种类似于平滑的效果，所以估计的值函数方差小。
- question: 什么是重要度采样？
  answer: 使用另外一种数据分布，来逼近所求分布的一种方法，在强化学习中通常和蒙特卡罗方法结合使用
- question: PPO 算法的改进
  answer: 避免在使用 important sampling 时由于在 θ 下的 pθ (at|st) 跟在 θ′ 下的 pθ′ (at|st) 差太多，导致 important sampling 结果偏差较大而采取的算法。具体来说就是在 training 的过程中增加一个 constrain，这个 constrain 对应着 θ 跟 θ′ output 的 action 的 KL divergence，来衡量 θ 与 θ′的相似程度。
- question: On Policy 的policy gradient有什么可改进之处，或者说有什么缺陷？
  answer: 经典 policy gradient 的大部分时间花在 sample data 处，即当我们的 agent 与环境做了交互后，我们就要进行 policy model 的更新。但是对于一个回合我们仅能更新 policy model 一次，更新完后我们就要花时间去重新 collect data，然后才能再次进行如上的更新。
- question: 怎么解决目标值总是太大的问题呢？
  answer: 在 Double DQN 里面，选动作的 Q-function 跟算值的 Q-function 不是同一个。在原来的 DQN 里面，你穷举所有的 a，把每一个 a 都带进去，看哪一个 a 可以给你的 Q值最高，那你就把那个 Q 值加上rt。但是在 Double DQN 里面，你有两个 Q-network 1)  第一个 Q-network Q 决定哪一个动作的 Q 值最大（你把所有的 a 带入 Q 中，看看哪一个 Q 值最大）。2) 你决定你的动作以后，你的 Q 值是用 Q′ 算出来的。
- question: 什么是 TD Target， 什么是 TD error
  answer: R + gamma*Qmax, R + gamma*Qmax - Q



