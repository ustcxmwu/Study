- question: 什么是强化学习？跟监督学习和非监督学习的区别是什么？
  answer: 强化学习侧重从交互中获取奖励或者惩罚来指导行动
- question: MDP性质，MDP组成
  answer: 无后效性，五元组（S,A,R,T, gamma)
- question: Model based 和 Model free的区别
  answer: 状态转移概率是否已知或者需要
- question: 强化学习的损失函数是什么？和深度学习的损失函数有何关系？
  answer: 强化学习的损失函数是使奖励和的期望最大；深度学习中的损失函数是使预测值和标签之间的差异最小化。
- question: 什么是 on-policy 和 off-policy，举例说明
  answer: 在线策略用于学习和用于采样的是同一个策略，离线策略中行动策略用来采样，目标策略是用来学习的。在线策略不学习最优动作而是学习一个接近最优动作同时又能继续探索的动作，离线策略直接学习最优动作。
- question: 简述蒙特卡罗估计值函数(MC)算法
  answer: MC算法是model-free的学习方法, 它从”经验“中学习价值函数和最优策略，”经验“是指多幕采样数据，MC通过平均样本的回报在大数定律的保证下进行策略估计，然后采用柔性策略进行MC控制。MC算法是深度采样更新，它没有使用自举法。
- question: 简述时间差分(TD)算法
  answer: TD算法和MC算法一样可以从和环境互动的经验中学习策略而不依赖环境的动态特性，TD和DP一样都采用的自举法，是采样更新。和MC不同的是TD算法采样深度没有那么深，它不是一个完全的采样，TD的策略评估是根据它直接得到的后继状态节点的单次样本转移来更新的，换言之它不需要等到一幕完全结束而是可以立刻进行学习。它采用后继状态的价值和沿途的收益进行更新，TD控制有Sarsa、期望Sarsa和Q学习。
- question: value-based和policy-based的区别是什么
  answer:
    - 处理的action space不同：value-based适合处理的action space低维离散的，policy-based适合处理连续的action space。
    - 针对action的价值输出不同：value-based计算出每个action的价值，policy-based一般情况下只给出较价值较高的actions。
    - 更新频率不同：value-based每个action执行都可以更新，policy-based 每个episode完成之后才能更新一次。
- question: DQN都有哪些变种？
  answer:
    - Double-DQN：将动作选择和价值估计分开，避免价值过高估计。
    - Dueling-DQN：将Q值分解为状态价值和优势函数，得到更多有用信息。
    - Prioritized Replay Buffer：将经验池中的经验按照优先级进行采样。
- question: 简述Double DQN原理？
  answer:
- question: 什么是偏差和方差
  answer: 偏差是真实值和预测值之间的偏离程度；方差是预测值得分散程度，即越分散，方差越大；
- question: 什么是预测问题
  answer:
    - 输入 MDP(S, A, R, P, gamma) 和策略 pi
    - 输出 最优状态值函数 V, 或者最优状态动作值函数 Q
- question: 什么是控制问题
  answer:
    - 输入 MDP(S, A, R, P, gamma)
    - 输出 最优状态值函数 V 或者最优状态动作值函数Q, 和最优策略 pi

