{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-22T08:09:39.974663Z",
     "start_time": "2024-11-22T08:09:16.644746Z"
    }
   },
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from tensordict.nn import TensorDictModule\n",
    "from tensordict.nn.distributions import NormalParamExtractor\n",
    "from torch import nn\n",
    "\n",
    "from torchrl.collectors import SyncDataCollector\n",
    "from torchrl.data.replay_buffers import ReplayBuffer\n",
    "from torchrl.data.replay_buffers.samplers import SamplerWithoutReplacement\n",
    "from torchrl.data.replay_buffers.storages import LazyTensorStorage\n",
    "from torchrl.envs import (\n",
    "    Compose,\n",
    "    DoubleToFloat,\n",
    "    ObservationNorm,\n",
    "    StepCounter,\n",
    "    TransformedEnv,\n",
    ")\n",
    "from torchrl.envs.libs.gym import GymEnv\n",
    "from torchrl.envs.utils import check_env_specs, ExplorationType, set_exploration_type\n",
    "from torchrl.modules import ProbabilisticActor, TanhNormal, ValueOperator\n",
    "from torchrl.objectives import ClipPPOLoss\n",
    "from torchrl.objectives.value import GAE\n",
    "from tqdm import tqdm\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T08:09:43.786850Z",
     "start_time": "2024-11-22T08:09:43.783090Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# import multiprocessing\n",
    "#\n",
    "# is_fork = multiprocessing.get_start_method() == \"fork\"\n",
    "# device = (\n",
    "#     torch.device(0)\n",
    "#     if torch.cuda.is_available() and not is_fork\n",
    "#     else torch.device(\"cpu\")\n",
    "# )\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "num_cells = 256  # number of cells in each layer i.e. output dim.\n",
    "lr = 3e-4\n",
    "max_grad_norm = 1.0"
   ],
   "id": "6a6322de5b54e6cf",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T08:09:45.165930Z",
     "start_time": "2024-11-22T08:09:45.163105Z"
    }
   },
   "cell_type": "code",
   "source": [
    "frames_per_batch = 1000\n",
    "# For a complete training, bring the number of frames up to 1M\n",
    "total_frames = 10_000"
   ],
   "id": "dc3d9a71805c259",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T08:09:46.253806Z",
     "start_time": "2024-11-22T08:09:46.251001Z"
    }
   },
   "cell_type": "code",
   "source": [
    "sub_batch_size = 64  # cardinality of the sub-samples gathered from the current data in the inner loop\n",
    "num_epochs = 10  # optimization steps per batch of data collected\n",
    "clip_epsilon = (\n",
    "    0.2  # clip value for PPO loss: see the equation in the intro for more context.\n",
    ")\n",
    "gamma = 0.99\n",
    "lmbda = 0.95\n",
    "entropy_eps = 1e-4"
   ],
   "id": "260485c5f8f39606",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T08:09:47.455518Z",
     "start_time": "2024-11-22T08:09:47.400777Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import gymnasium as gym\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "observation, info = env.reset(seed=42)\n",
    "for _ in range(1000):\n",
    "    action = env.action_space.sample()\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "    if terminated or truncated:\n",
    "        observation, info = env.reset()\n",
    "env.close()"
   ],
   "id": "869c321912abdf09",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T08:09:49.752935Z",
     "start_time": "2024-11-22T08:09:48.395785Z"
    }
   },
   "cell_type": "code",
   "source": [
    "base_env = GymEnv(\"InvertedDoublePendulum-v4\", device=device)\n",
    "# base_env = GymEnv(\"InvertedDoublePendulum-v4\")"
   ],
   "id": "98da5b99dcdc02b0",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T08:09:50.836571Z",
     "start_time": "2024-11-22T08:09:50.463045Z"
    }
   },
   "cell_type": "code",
   "source": [
    "env = TransformedEnv(\n",
    "    base_env,\n",
    "    Compose(\n",
    "        # normalize observations\n",
    "        ObservationNorm(in_keys=[\"observation\"]),\n",
    "        DoubleToFloat(),\n",
    "        StepCounter(),\n",
    "    ),\n",
    ")\n",
    "\n",
    "env.transform[0].init_stats(num_iter=1000, reduce_dim=0, cat_dim=0)\n",
    "print(\"normalization constant shape:\", env.transform[0].loc.shape)\n",
    "print(\"observation_spec:\", env.observation_spec)\n",
    "print(\"reward_spec:\", env.reward_spec)\n",
    "print(\"input_spec:\", env.input_spec)\n",
    "print(\"action_spec (as defined by input_spec):\", env.action_spec)"
   ],
   "id": "2496b5b65ea9a6be",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalization constant shape: torch.Size([11])\n",
      "observation_spec: CompositeSpec(\n",
      "    observation: UnboundedContinuousTensorSpec(\n",
      "        shape=torch.Size([11]),\n",
      "        space=None,\n",
      "        device=cpu,\n",
      "        dtype=torch.float32,\n",
      "        domain=continuous),\n",
      "    step_count: BoundedTensorSpec(\n",
      "        shape=torch.Size([1]),\n",
      "        space=ContinuousBox(\n",
      "            low=Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.int64, contiguous=True),\n",
      "            high=Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.int64, contiguous=True)),\n",
      "        device=cpu,\n",
      "        dtype=torch.int64,\n",
      "        domain=continuous), device=cpu, shape=torch.Size([]))\n",
      "reward_spec: UnboundedContinuousTensorSpec(\n",
      "    shape=torch.Size([1]),\n",
      "    space=ContinuousBox(\n",
      "        low=Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.float32, contiguous=True),\n",
      "        high=Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.float32, contiguous=True)),\n",
      "    device=cpu,\n",
      "    dtype=torch.float32,\n",
      "    domain=continuous)\n",
      "input_spec: CompositeSpec(\n",
      "    full_state_spec: CompositeSpec(\n",
      "        step_count: BoundedTensorSpec(\n",
      "            shape=torch.Size([1]),\n",
      "            space=ContinuousBox(\n",
      "                low=Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.int64, contiguous=True),\n",
      "                high=Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.int64, contiguous=True)),\n",
      "            device=cpu,\n",
      "            dtype=torch.int64,\n",
      "            domain=continuous), device=cpu, shape=torch.Size([])),\n",
      "    full_action_spec: CompositeSpec(\n",
      "        action: BoundedTensorSpec(\n",
      "            shape=torch.Size([1]),\n",
      "            space=ContinuousBox(\n",
      "                low=Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.float32, contiguous=True),\n",
      "                high=Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.float32, contiguous=True)),\n",
      "            device=cpu,\n",
      "            dtype=torch.float32,\n",
      "            domain=continuous), device=cpu, shape=torch.Size([])), device=cpu, shape=torch.Size([]))\n",
      "action_spec (as defined by input_spec): BoundedTensorSpec(\n",
      "    shape=torch.Size([1]),\n",
      "    space=ContinuousBox(\n",
      "        low=Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.float32, contiguous=True),\n",
      "        high=Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.float32, contiguous=True)),\n",
      "    device=cpu,\n",
      "    dtype=torch.float32,\n",
      "    domain=continuous)\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T08:09:51.916817Z",
     "start_time": "2024-11-22T08:09:51.892638Z"
    }
   },
   "cell_type": "code",
   "source": "check_env_specs(env)",
   "id": "7417c3afb9077e74",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-22 16:09:51,914 [torchrl][INFO] check_env_specs succeeded!\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T08:09:53.046039Z",
     "start_time": "2024-11-22T08:09:53.036548Z"
    }
   },
   "cell_type": "code",
   "source": [
    "rollout = env.rollout(3)\n",
    "print(\"rollout of three steps:\", rollout)\n",
    "print(\"Shape of the rollout TensorDict:\", rollout.batch_size)"
   ],
   "id": "f5f9a4dce3f42ef",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rollout of three steps: TensorDict(\n",
      "    fields={\n",
      "        action: Tensor(shape=torch.Size([3, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        done: Tensor(shape=torch.Size([3, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        next: TensorDict(\n",
      "            fields={\n",
      "                done: Tensor(shape=torch.Size([3, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "                observation: Tensor(shape=torch.Size([3, 11]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                reward: Tensor(shape=torch.Size([3, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                step_count: Tensor(shape=torch.Size([3, 1]), device=cpu, dtype=torch.int64, is_shared=False),\n",
      "                terminated: Tensor(shape=torch.Size([3, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "                truncated: Tensor(shape=torch.Size([3, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
      "            batch_size=torch.Size([3]),\n",
      "            device=cpu,\n",
      "            is_shared=False),\n",
      "        observation: Tensor(shape=torch.Size([3, 11]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        step_count: Tensor(shape=torch.Size([3, 1]), device=cpu, dtype=torch.int64, is_shared=False),\n",
      "        terminated: Tensor(shape=torch.Size([3, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        truncated: Tensor(shape=torch.Size([3, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
      "    batch_size=torch.Size([3]),\n",
      "    device=cpu,\n",
      "    is_shared=False)\n",
      "Shape of the rollout TensorDict: torch.Size([3])\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T08:09:54.711953Z",
     "start_time": "2024-11-22T08:09:54.705116Z"
    }
   },
   "cell_type": "code",
   "source": [
    "actor_net = nn.Sequential(\n",
    "    nn.LazyLinear(num_cells, device=device),\n",
    "    nn.Tanh(),\n",
    "    nn.LazyLinear(num_cells, device=device),\n",
    "    nn.Tanh(),\n",
    "    nn.LazyLinear(num_cells, device=device),\n",
    "    nn.Tanh(),\n",
    "    nn.LazyLinear(2 * env.action_spec.shape[-1], device=device),\n",
    "    NormalParamExtractor(),\n",
    ")"
   ],
   "id": "7e2660377d2f2961",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wuxiaomin/PycharmProjects/Study/.venv/lib/python3.10/site-packages/torch/nn/modules/lazy.py:181: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T08:09:56.089767Z",
     "start_time": "2024-11-22T08:09:56.085255Z"
    }
   },
   "cell_type": "code",
   "source": [
    "policy_module = TensorDictModule(\n",
    "    actor_net, in_keys=[\"observation\"], out_keys=[\"loc\", \"scale\"]\n",
    ")"
   ],
   "id": "a8b4d9a3b2ef19fd",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T08:09:57.155558Z",
     "start_time": "2024-11-22T08:09:57.153471Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# policy_module = ProbabilisticActor(\n",
    "#     module=policy_module,\n",
    "#     spec=env.action_spec,\n",
    "#     in_keys=[\"loc\", \"scale\"],\n",
    "#     distribution_class=TanhNormal,\n",
    "#     distribution_kwargs={\n",
    "#         \"low\": env.action_spec.space.low,\n",
    "#         \"high\": env.action_spec.space.high,\n",
    "#     },\n",
    "#     return_log_prob=True,\n",
    "#     # we'll need the log-prob for the numerator of the importance weights\n",
    "# )"
   ],
   "id": "8d053896e701e347",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T08:09:58.223823Z",
     "start_time": "2024-11-22T08:09:58.216656Z"
    }
   },
   "cell_type": "code",
   "source": [
    "value_net = nn.Sequential(\n",
    "    nn.LazyLinear(num_cells, device=device),\n",
    "    nn.Tanh(),\n",
    "    nn.LazyLinear(num_cells, device=device),\n",
    "    nn.Tanh(),\n",
    "    nn.LazyLinear(num_cells, device=device),\n",
    "    nn.Tanh(),\n",
    "    nn.LazyLinear(1, device=device),\n",
    ")\n",
    "\n",
    "value_module = ValueOperator(\n",
    "    module=value_net,\n",
    "    in_keys=[\"observation\"],\n",
    ")"
   ],
   "id": "bc01ecf333972422",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T08:09:59.463832Z",
     "start_time": "2024-11-22T08:09:59.453892Z"
    }
   },
   "cell_type": "code",
   "source": "env.reset()",
   "id": "e2a23cffe14eb643",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorDict(\n",
       "    fields={\n",
       "        done: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
       "        observation: Tensor(shape=torch.Size([11]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        step_count: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.int64, is_shared=False),\n",
       "        terminated: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
       "        truncated: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
       "    batch_size=torch.Size([]),\n",
       "    device=cpu,\n",
       "    is_shared=False)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T08:10:00.577637Z",
     "start_time": "2024-11-22T08:10:00.528418Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"Running policy:\", policy_module(env.reset()))\n",
    "print(\"Running value:\", value_module(env.reset()))"
   ],
   "id": "57b04c2dcbbcbe59",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running policy: TensorDict(\n",
      "    fields={\n",
      "        done: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        loc: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        observation: Tensor(shape=torch.Size([11]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        scale: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        step_count: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.int64, is_shared=False),\n",
      "        terminated: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        truncated: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
      "    batch_size=torch.Size([]),\n",
      "    device=cpu,\n",
      "    is_shared=False)\n",
      "Running value: TensorDict(\n",
      "    fields={\n",
      "        done: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        observation: Tensor(shape=torch.Size([11]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        state_value: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        step_count: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.int64, is_shared=False),\n",
      "        terminated: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        truncated: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
      "    batch_size=torch.Size([]),\n",
      "    device=cpu,\n",
      "    is_shared=False)\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T08:10:02.085302Z",
     "start_time": "2024-11-22T08:10:02.066015Z"
    }
   },
   "cell_type": "code",
   "source": [
    "collector = SyncDataCollector(\n",
    "    env,\n",
    "    policy_module,\n",
    "    frames_per_batch=frames_per_batch,\n",
    "    total_frames=total_frames,\n",
    "    split_trajs=False,\n",
    "    device=device,\n",
    ")"
   ],
   "id": "d11fc6bfa87a1df0",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T08:10:03.458498Z",
     "start_time": "2024-11-22T08:10:03.424105Z"
    }
   },
   "cell_type": "code",
   "source": [
    "replay_buffer = ReplayBuffer(\n",
    "    storage=LazyTensorStorage(max_size=frames_per_batch),\n",
    "    sampler=SamplerWithoutReplacement(),\n",
    ")"
   ],
   "id": "62c93cc9ed01d62a",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T08:10:08.470188Z",
     "start_time": "2024-11-22T08:10:05.515236Z"
    }
   },
   "cell_type": "code",
   "source": [
    "advantage_module = GAE(\n",
    "    gamma=gamma, lmbda=lmbda, value_network=value_module, average_gae=True\n",
    ")\n",
    "\n",
    "loss_module = ClipPPOLoss(\n",
    "    actor_network=policy_module,\n",
    "    critic_network=value_module,\n",
    "    clip_epsilon=clip_epsilon,\n",
    "    entropy_bonus=bool(entropy_eps),\n",
    "    entropy_coef=entropy_eps,\n",
    "    # these keys match by default but we set this for completeness\n",
    "    critic_coef=1.0,\n",
    "    loss_critic_type=\"smooth_l1\",\n",
    ")\n",
    "\n",
    "optim = torch.optim.Adam(loss_module.parameters(), lr)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optim, total_frames // frames_per_batch, 0.0\n",
    ")"
   ],
   "id": "4fbabf7ddde0f1c2",
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "This version of jaxlib was built using AVX instructions, which your CPU and/or operating system do not support. This error is frequently encountered on macOS when running an x86 Python installation on ARM hardware. In this case, try installing an ARM build of Python. Otherwise, you may be able work around this issue by building jaxlib from source.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[18], line 16\u001B[0m\n\u001B[1;32m      1\u001B[0m advantage_module \u001B[38;5;241m=\u001B[39m GAE(\n\u001B[1;32m      2\u001B[0m     gamma\u001B[38;5;241m=\u001B[39mgamma, lmbda\u001B[38;5;241m=\u001B[39mlmbda, value_network\u001B[38;5;241m=\u001B[39mvalue_module, average_gae\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m      3\u001B[0m )\n\u001B[1;32m      5\u001B[0m loss_module \u001B[38;5;241m=\u001B[39m ClipPPOLoss(\n\u001B[1;32m      6\u001B[0m     actor_network\u001B[38;5;241m=\u001B[39mpolicy_module,\n\u001B[1;32m      7\u001B[0m     critic_network\u001B[38;5;241m=\u001B[39mvalue_module,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     13\u001B[0m     loss_critic_type\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msmooth_l1\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m     14\u001B[0m )\n\u001B[0;32m---> 16\u001B[0m optim \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptim\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mAdam\u001B[49m\u001B[43m(\u001B[49m\u001B[43mloss_module\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mparameters\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlr\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     17\u001B[0m scheduler \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39moptim\u001B[38;5;241m.\u001B[39mlr_scheduler\u001B[38;5;241m.\u001B[39mCosineAnnealingLR(\n\u001B[1;32m     18\u001B[0m     optim, total_frames \u001B[38;5;241m/\u001B[39m\u001B[38;5;241m/\u001B[39m frames_per_batch, \u001B[38;5;241m0.0\u001B[39m\n\u001B[1;32m     19\u001B[0m )\n",
      "File \u001B[0;32m~/PycharmProjects/Study/.venv/lib/python3.10/site-packages/torch/optim/adam.py:45\u001B[0m, in \u001B[0;36mAdam.__init__\u001B[0;34m(self, params, lr, betas, eps, weight_decay, amsgrad, foreach, maximize, capturable, differentiable, fused)\u001B[0m\n\u001B[1;32m     39\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mInvalid weight_decay value: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mweight_decay\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     41\u001B[0m defaults \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mdict\u001B[39m(lr\u001B[38;5;241m=\u001B[39mlr, betas\u001B[38;5;241m=\u001B[39mbetas, eps\u001B[38;5;241m=\u001B[39meps,\n\u001B[1;32m     42\u001B[0m                 weight_decay\u001B[38;5;241m=\u001B[39mweight_decay, amsgrad\u001B[38;5;241m=\u001B[39mamsgrad,\n\u001B[1;32m     43\u001B[0m                 maximize\u001B[38;5;241m=\u001B[39mmaximize, foreach\u001B[38;5;241m=\u001B[39mforeach, capturable\u001B[38;5;241m=\u001B[39mcapturable,\n\u001B[1;32m     44\u001B[0m                 differentiable\u001B[38;5;241m=\u001B[39mdifferentiable, fused\u001B[38;5;241m=\u001B[39mfused)\n\u001B[0;32m---> 45\u001B[0m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__init__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdefaults\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m fused:\n\u001B[1;32m     48\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m differentiable:\n",
      "File \u001B[0;32m~/PycharmProjects/Study/.venv/lib/python3.10/site-packages/torch/optim/optimizer.py:278\u001B[0m, in \u001B[0;36mOptimizer.__init__\u001B[0;34m(self, params, defaults)\u001B[0m\n\u001B[1;32m    275\u001B[0m     param_groups \u001B[38;5;241m=\u001B[39m [{\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mparams\u001B[39m\u001B[38;5;124m'\u001B[39m: param_groups}]\n\u001B[1;32m    277\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m param_group \u001B[38;5;129;01min\u001B[39;00m param_groups:\n\u001B[0;32m--> 278\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43madd_param_group\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcast\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mdict\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mparam_group\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    280\u001B[0m \u001B[38;5;66;03m# Allows _cuda_graph_capture_health_check to rig a poor man's TORCH_WARN_ONCE in python,\u001B[39;00m\n\u001B[1;32m    281\u001B[0m \u001B[38;5;66;03m# which I don't think exists\u001B[39;00m\n\u001B[1;32m    282\u001B[0m \u001B[38;5;66;03m# https://github.com/pytorch/pytorch/issues/72948\u001B[39;00m\n\u001B[1;32m    283\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_warned_capturable_if_run_uncaptured \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "File \u001B[0;32m~/PycharmProjects/Study/.venv/lib/python3.10/site-packages/torch/_compile.py:22\u001B[0m, in \u001B[0;36m_disable_dynamo.<locals>.inner\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     20\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(fn)\n\u001B[1;32m     21\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21minner\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m---> 22\u001B[0m     \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_dynamo\u001B[39;00m\n\u001B[1;32m     24\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m torch\u001B[38;5;241m.\u001B[39m_dynamo\u001B[38;5;241m.\u001B[39mdisable(fn, recursive)(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m~/PycharmProjects/Study/.venv/lib/python3.10/site-packages/torch/_dynamo/__init__.py:2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m allowed_functions, convert_frame, eval_frame, resume_execution\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mbackends\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mregistry\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m list_backends, lookup_backend, register_backend\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcode_context\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m code_context\n",
      "File \u001B[0;32m~/PycharmProjects/Study/.venv/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:45\u001B[0m\n\u001B[1;32m     32\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mbytecode_transformation\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[1;32m     33\u001B[0m     check_inst_exn_tab_entries_valid,\n\u001B[1;32m     34\u001B[0m     Instruction,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     37\u001B[0m     transform_code_object,\n\u001B[1;32m     38\u001B[0m )\n\u001B[1;32m     39\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcache_size\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[1;32m     40\u001B[0m     CacheSizeRelevantForFrame,\n\u001B[1;32m     41\u001B[0m     compute_cache_size,\n\u001B[1;32m     42\u001B[0m     exceeds_cache_size_limit,\n\u001B[1;32m     43\u001B[0m     is_recompilation,\n\u001B[1;32m     44\u001B[0m )\n\u001B[0;32m---> 45\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01meval_frame\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m always_optimize_code_objects, skip_code, TorchPatcher\n\u001B[1;32m     46\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mexc\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[1;32m     47\u001B[0m     augment_exc_message,\n\u001B[1;32m     48\u001B[0m     BackendCompilerFailed,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     54\u001B[0m     Unsupported,\n\u001B[1;32m     55\u001B[0m )\n\u001B[1;32m     56\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mguards\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[1;32m     57\u001B[0m     CheckFunctionManager,\n\u001B[1;32m     58\u001B[0m     get_and_maybe_log_recompilation_reason,\n\u001B[1;32m     59\u001B[0m     GuardedCode,\n\u001B[1;32m     60\u001B[0m )\n",
      "File \u001B[0;32m~/PycharmProjects/Study/.venv/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:69\u001B[0m\n\u001B[1;32m     66\u001B[0m             \u001B[38;5;28;01mcontinue\u001B[39;00m\n\u001B[1;32m     67\u001B[0m         \u001B[38;5;28mglobals\u001B[39m()[name] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mgetattr\u001B[39m(torch\u001B[38;5;241m.\u001B[39m_C\u001B[38;5;241m.\u001B[39m_dynamo\u001B[38;5;241m.\u001B[39meval_frame, name)\n\u001B[0;32m---> 69\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m config, convert_frame, external_utils, skipfiles, utils\n\u001B[1;32m     70\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcode_context\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m code_context\n\u001B[1;32m     71\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mexc\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m CondOpArgsMismatchError, UserError, UserErrorType\n",
      "File \u001B[0;32m~/PycharmProjects/Study/.venv/lib/python3.10/site-packages/torch/_dynamo/skipfiles.py:39\u001B[0m\n\u001B[1;32m     36\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_content_store\u001B[39;00m\n\u001B[1;32m     37\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m getfile\n\u001B[0;32m---> 39\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mvariables\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mfunctions\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[1;32m     40\u001B[0m     NestedUserFunctionVariable,\n\u001B[1;32m     41\u001B[0m     UserFunctionVariable,\n\u001B[1;32m     42\u001B[0m     UserMethodVariable,\n\u001B[1;32m     43\u001B[0m )\n\u001B[1;32m     46\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m     47\u001B[0m \u001B[38;5;124;03mA note on skipfiles:\u001B[39;00m\n\u001B[1;32m     48\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     86\u001B[0m \u001B[38;5;124;03myou don't want to inline them.\u001B[39;00m\n\u001B[1;32m     87\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m     90\u001B[0m BUILTIN_SKIPLIST \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m     91\u001B[0m     abc,\n\u001B[1;32m     92\u001B[0m     collections,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    121\u001B[0m     _weakrefset,\n\u001B[1;32m    122\u001B[0m )\n",
      "File \u001B[0;32m~/PycharmProjects/Study/.venv/lib/python3.10/site-packages/torch/_dynamo/variables/__init__.py:26\u001B[0m\n\u001B[1;32m     14\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdicts\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[1;32m     15\u001B[0m     ConstDictVariable,\n\u001B[1;32m     16\u001B[0m     CustomizedDictVariable,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     19\u001B[0m     SetVariable,\n\u001B[1;32m     20\u001B[0m )\n\u001B[1;32m     21\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mfunctions\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[1;32m     22\u001B[0m     NestedUserFunctionVariable,\n\u001B[1;32m     23\u001B[0m     UserFunctionVariable,\n\u001B[1;32m     24\u001B[0m     UserMethodVariable,\n\u001B[1;32m     25\u001B[0m )\n\u001B[0;32m---> 26\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mhigher_order_ops\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m TorchHigherOrderOperatorVariable\n\u001B[1;32m     27\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01miter\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[1;32m     28\u001B[0m     CountIteratorVariable,\n\u001B[1;32m     29\u001B[0m     CycleIteratorVariable,\n\u001B[1;32m     30\u001B[0m     IteratorVariable,\n\u001B[1;32m     31\u001B[0m     RepeatIteratorVariable,\n\u001B[1;32m     32\u001B[0m )\n\u001B[1;32m     33\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlazy\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m LazyVariableTracker\n",
      "File \u001B[0;32m~/PycharmProjects/Study/.venv/lib/python3.10/site-packages/torch/_dynamo/variables/higher_order_ops.py:11\u001B[0m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mfx\u001B[39;00m\n\u001B[1;32m     10\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mnn\u001B[39;00m\n\u001B[0;32m---> 11\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01monnx\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01moperators\u001B[39;00m\n\u001B[1;32m     12\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_dispatch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m enable_python_dispatcher\n\u001B[1;32m     13\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_dynamo\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m deepcopy_to_fake_tensor, get_fake_value, get_real_value\n",
      "File \u001B[0;32m~/PycharmProjects/Study/.venv/lib/python3.10/site-packages/torch/onnx/__init__.py:46\u001B[0m\n\u001B[1;32m     33\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01merrors\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m CheckerError  \u001B[38;5;66;03m# Backwards compatibility\u001B[39;00m\n\u001B[1;32m     34\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[1;32m     35\u001B[0m     _optimize_graph,\n\u001B[1;32m     36\u001B[0m     _run_symbolic_function,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     43\u001B[0m     unregister_custom_op_symbolic,\n\u001B[1;32m     44\u001B[0m )\n\u001B[0;32m---> 46\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_internal\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mexporter\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (  \u001B[38;5;66;03m# usort:skip. needs to be last to avoid circular import\u001B[39;00m\n\u001B[1;32m     47\u001B[0m     DiagnosticOptions,\n\u001B[1;32m     48\u001B[0m     ExportOptions,\n\u001B[1;32m     49\u001B[0m     ONNXProgram,\n\u001B[1;32m     50\u001B[0m     ONNXProgramSerializer,\n\u001B[1;32m     51\u001B[0m     ONNXRuntimeOptions,\n\u001B[1;32m     52\u001B[0m     InvalidExportOptionsError,\n\u001B[1;32m     53\u001B[0m     OnnxExporterError,\n\u001B[1;32m     54\u001B[0m     OnnxRegistry,\n\u001B[1;32m     55\u001B[0m     dynamo_export,\n\u001B[1;32m     56\u001B[0m     enable_fake_mode,\n\u001B[1;32m     57\u001B[0m )\n\u001B[1;32m     59\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_internal\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01monnxruntime\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[1;32m     60\u001B[0m     is_onnxrt_backend_supported,\n\u001B[1;32m     61\u001B[0m     OrtBackend \u001B[38;5;28;01mas\u001B[39;00m _OrtBackend,\n\u001B[1;32m     62\u001B[0m     OrtBackendOptions \u001B[38;5;28;01mas\u001B[39;00m _OrtBackendOptions,\n\u001B[1;32m     63\u001B[0m     OrtExecutionProvider \u001B[38;5;28;01mas\u001B[39;00m _OrtExecutionProvider,\n\u001B[1;32m     64\u001B[0m )\n\u001B[1;32m     66\u001B[0m __all__ \u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m     67\u001B[0m     \u001B[38;5;66;03m# Modules\u001B[39;00m\n\u001B[1;32m     68\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msymbolic_helper\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    114\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mis_onnxrt_backend_supported\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    115\u001B[0m ]\n",
      "File \u001B[0;32m~/PycharmProjects/Study/.venv/lib/python3.10/site-packages/torch/onnx/_internal/exporter.py:44\u001B[0m\n\u001B[1;32m     42\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01monnx\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_internal\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m _beartype, io_adapter\n\u001B[1;32m     43\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01monnx\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_internal\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdiagnostics\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m infra\n\u001B[0;32m---> 44\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01monnx\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_internal\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mfx\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[1;32m     45\u001B[0m     decomposition_table,\n\u001B[1;32m     46\u001B[0m     patcher \u001B[38;5;28;01mas\u001B[39;00m patcher,\n\u001B[1;32m     47\u001B[0m     registration,\n\u001B[1;32m     48\u001B[0m     serialization \u001B[38;5;28;01mas\u001B[39;00m fx_serialization,\n\u001B[1;32m     49\u001B[0m )\n\u001B[1;32m     51\u001B[0m \u001B[38;5;66;03m# We can only import onnx from this module in a type-checking context to ensure that\u001B[39;00m\n\u001B[1;32m     52\u001B[0m \u001B[38;5;66;03m# 'import torch.onnx' continues to work without having 'onnx' installed. We fully\u001B[39;00m\n\u001B[1;32m     53\u001B[0m \u001B[38;5;66;03m# 'import onnx' inside of dynamo_export (by way of _assert_dependencies).\u001B[39;00m\n\u001B[1;32m     54\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m TYPE_CHECKING:\n",
      "File \u001B[0;32m~/PycharmProjects/Study/.venv/lib/python3.10/site-packages/torch/onnx/_internal/fx/__init__.py:1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpatcher\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ONNXTorchPatcher\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mserialization\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m save_model_with_external_data\n\u001B[1;32m      5\u001B[0m __all__ \u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m      6\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msave_model_with_external_data\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m      7\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mONNXTorchPatcher\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m      8\u001B[0m ]\n",
      "File \u001B[0;32m~/PycharmProjects/Study/.venv/lib/python3.10/site-packages/torch/onnx/_internal/fx/patcher.py:11\u001B[0m\n\u001B[1;32m      8\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m      9\u001B[0m     \u001B[38;5;66;03m# safetensors is not an exporter requirement, but needed for some huggingface models\u001B[39;00m\n\u001B[1;32m     10\u001B[0m     \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01msafetensors\u001B[39;00m  \u001B[38;5;66;03m# type: ignore[import]  # noqa: F401\u001B[39;00m\n\u001B[0;32m---> 11\u001B[0m     \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtransformers\u001B[39;00m  \u001B[38;5;66;03m# type: ignore[import]\u001B[39;00m\n\u001B[1;32m     12\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msafetensors\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m torch \u001B[38;5;28;01mas\u001B[39;00m safetensors_torch  \u001B[38;5;66;03m# noqa: F401\u001B[39;00m\n\u001B[1;32m     14\u001B[0m     has_safetensors_and_transformers \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "File \u001B[0;32m~/PycharmProjects/Study/.venv/lib/python3.10/site-packages/transformers/__init__.py:26\u001B[0m\n\u001B[1;32m     23\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtyping\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m TYPE_CHECKING\n\u001B[1;32m     25\u001B[0m \u001B[38;5;66;03m# Check the dependencies satisfy the minimal versions required.\u001B[39;00m\n\u001B[0;32m---> 26\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m dependency_versions_check\n\u001B[1;32m     27\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[1;32m     28\u001B[0m     OptionalDependencyNotAvailable,\n\u001B[1;32m     29\u001B[0m     _LazyModule,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     48\u001B[0m     logging,\n\u001B[1;32m     49\u001B[0m )\n\u001B[1;32m     52\u001B[0m logger \u001B[38;5;241m=\u001B[39m logging\u001B[38;5;241m.\u001B[39mget_logger(\u001B[38;5;18m__name__\u001B[39m)  \u001B[38;5;66;03m# pylint: disable=invalid-name\u001B[39;00m\n",
      "File \u001B[0;32m~/PycharmProjects/Study/.venv/lib/python3.10/site-packages/transformers/dependency_versions_check.py:16\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Copyright 2020 The HuggingFace Team. All rights reserved.\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     12\u001B[0m \u001B[38;5;66;03m# See the License for the specific language governing permissions and\u001B[39;00m\n\u001B[1;32m     13\u001B[0m \u001B[38;5;66;03m# limitations under the License.\u001B[39;00m\n\u001B[1;32m     15\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdependency_versions_table\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m deps\n\u001B[0;32m---> 16\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mversions\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m require_version, require_version_core\n\u001B[1;32m     19\u001B[0m \u001B[38;5;66;03m# define which module versions we always want to check at run time\u001B[39;00m\n\u001B[1;32m     20\u001B[0m \u001B[38;5;66;03m# (usually the ones defined in `install_requires` in setup.py)\u001B[39;00m\n\u001B[1;32m     21\u001B[0m \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[1;32m     22\u001B[0m \u001B[38;5;66;03m# order specific notes:\u001B[39;00m\n\u001B[1;32m     23\u001B[0m \u001B[38;5;66;03m# - tqdm must be checked before tokenizers\u001B[39;00m\n\u001B[1;32m     25\u001B[0m pkgs_to_check_at_runtime \u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m     26\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpython\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m     27\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtqdm\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     37\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpyyaml\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m     38\u001B[0m ]\n",
      "File \u001B[0;32m~/PycharmProjects/Study/.venv/lib/python3.10/site-packages/transformers/utils/__init__.py:33\u001B[0m\n\u001B[1;32m     24\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mconstants\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD, IMAGENET_STANDARD_MEAN, IMAGENET_STANDARD_STD\n\u001B[1;32m     25\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdoc\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[1;32m     26\u001B[0m     add_code_sample_docstrings,\n\u001B[1;32m     27\u001B[0m     add_end_docstrings,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     31\u001B[0m     replace_return_docstrings,\n\u001B[1;32m     32\u001B[0m )\n\u001B[0;32m---> 33\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mgeneric\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[1;32m     34\u001B[0m     ContextManagers,\n\u001B[1;32m     35\u001B[0m     ExplicitEnum,\n\u001B[1;32m     36\u001B[0m     ModelOutput,\n\u001B[1;32m     37\u001B[0m     PaddingStrategy,\n\u001B[1;32m     38\u001B[0m     TensorType,\n\u001B[1;32m     39\u001B[0m     add_model_info_to_auto_map,\n\u001B[1;32m     40\u001B[0m     cached_property,\n\u001B[1;32m     41\u001B[0m     can_return_loss,\n\u001B[1;32m     42\u001B[0m     expand_dims,\n\u001B[1;32m     43\u001B[0m     find_labels,\n\u001B[1;32m     44\u001B[0m     flatten_dict,\n\u001B[1;32m     45\u001B[0m     infer_framework,\n\u001B[1;32m     46\u001B[0m     is_jax_tensor,\n\u001B[1;32m     47\u001B[0m     is_numpy_array,\n\u001B[1;32m     48\u001B[0m     is_tensor,\n\u001B[1;32m     49\u001B[0m     is_tf_symbolic_tensor,\n\u001B[1;32m     50\u001B[0m     is_tf_tensor,\n\u001B[1;32m     51\u001B[0m     is_torch_device,\n\u001B[1;32m     52\u001B[0m     is_torch_dtype,\n\u001B[1;32m     53\u001B[0m     is_torch_tensor,\n\u001B[1;32m     54\u001B[0m     reshape,\n\u001B[1;32m     55\u001B[0m     squeeze,\n\u001B[1;32m     56\u001B[0m     strtobool,\n\u001B[1;32m     57\u001B[0m     tensor_size,\n\u001B[1;32m     58\u001B[0m     to_numpy,\n\u001B[1;32m     59\u001B[0m     to_py_obj,\n\u001B[1;32m     60\u001B[0m     transpose,\n\u001B[1;32m     61\u001B[0m     working_or_temp_dir,\n\u001B[1;32m     62\u001B[0m )\n\u001B[1;32m     63\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mhub\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[1;32m     64\u001B[0m     CLOUDFRONT_DISTRIB_PREFIX,\n\u001B[1;32m     65\u001B[0m     HF_MODULES_CACHE,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     91\u001B[0m     try_to_load_from_cache,\n\u001B[1;32m     92\u001B[0m )\n\u001B[1;32m     93\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mimport_utils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[1;32m     94\u001B[0m     ACCELERATE_MIN_VERSION,\n\u001B[1;32m     95\u001B[0m     ENV_VARS_TRUE_AND_AUTO_VALUES,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    207\u001B[0m     torch_only_method,\n\u001B[1;32m    208\u001B[0m )\n",
      "File \u001B[0;32m~/PycharmProjects/Study/.venv/lib/python3.10/site-packages/transformers/utils/generic.py:42\u001B[0m\n\u001B[1;32m     31\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mimport_utils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[1;32m     32\u001B[0m     get_torch_version,\n\u001B[1;32m     33\u001B[0m     is_flax_available,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     37\u001B[0m     is_torch_fx_proxy,\n\u001B[1;32m     38\u001B[0m )\n\u001B[1;32m     41\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_flax_available():\n\u001B[0;32m---> 42\u001B[0m     \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mjax\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mnumpy\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mjnp\u001B[39;00m\n\u001B[1;32m     45\u001B[0m \u001B[38;5;28;01mclass\u001B[39;00m \u001B[38;5;21;01mcached_property\u001B[39;00m(\u001B[38;5;28mproperty\u001B[39m):\n\u001B[1;32m     46\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m     47\u001B[0m \u001B[38;5;124;03m    Descriptor that mimics @property but caches output in member variable.\u001B[39;00m\n\u001B[1;32m     48\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     51\u001B[0m \u001B[38;5;124;03m    Built-in in functools from Python 3.8.\u001B[39;00m\n\u001B[1;32m     52\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n",
      "File \u001B[0;32m~/PycharmProjects/Study/.venv/lib/python3.10/site-packages/jax/__init__.py:25\u001B[0m\n\u001B[1;32m     22\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mjax\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mversion\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m __version_info__ \u001B[38;5;28;01mas\u001B[39;00m __version_info__\n\u001B[1;32m     24\u001B[0m \u001B[38;5;66;03m# Set Cloud TPU env vars if necessary before transitively loading C++ backend\u001B[39;00m\n\u001B[0;32m---> 25\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mjax\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_src\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcloud_tpu_init\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m cloud_tpu_init \u001B[38;5;28;01mas\u001B[39;00m _cloud_tpu_init\n\u001B[1;32m     26\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m     27\u001B[0m   _cloud_tpu_init()\n",
      "File \u001B[0;32m~/PycharmProjects/Study/.venv/lib/python3.10/site-packages/jax/_src/cloud_tpu_init.py:17\u001B[0m\n\u001B[1;32m     15\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mos\u001B[39;00m\n\u001B[1;32m     16\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mjax\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m version\n\u001B[0;32m---> 17\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mjax\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_src\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m config\n\u001B[1;32m     18\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mjax\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_src\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m hardware_utils\n\u001B[1;32m     20\u001B[0m running_in_cloud_tpu_vm: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "File \u001B[0;32m~/PycharmProjects/Study/.venv/lib/python3.10/site-packages/jax/_src/config.py:27\u001B[0m\n\u001B[1;32m     24\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mthreading\u001B[39;00m\n\u001B[1;32m     25\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtyping\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Any, Generic, NamedTuple, NoReturn, Protocol, TypeVar, cast\n\u001B[0;32m---> 27\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mjax\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_src\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m lib\n\u001B[1;32m     28\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mjax\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_src\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlib\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m guard_lib\n\u001B[1;32m     29\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mjax\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_src\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlib\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m jax_jit\n",
      "File \u001B[0;32m~/PycharmProjects/Study/.venv/lib/python3.10/site-packages/jax/_src/lib/__init__.py:84\u001B[0m\n\u001B[1;32m     80\u001B[0m \u001B[38;5;66;03m# Before importing any C compiled modules from jaxlib, first import the CPU\u001B[39;00m\n\u001B[1;32m     81\u001B[0m \u001B[38;5;66;03m# feature guard module to verify that jaxlib was compiled in a way that only\u001B[39;00m\n\u001B[1;32m     82\u001B[0m \u001B[38;5;66;03m# uses instructions that are present on this machine.\u001B[39;00m\n\u001B[1;32m     83\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mjaxlib\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcpu_feature_guard\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mcpu_feature_guard\u001B[39;00m\n\u001B[0;32m---> 84\u001B[0m \u001B[43mcpu_feature_guard\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcheck_cpu_features\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     86\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mjaxlib\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mutils\u001B[39;00m  \u001B[38;5;66;03m# noqa: F401\u001B[39;00m\n\u001B[1;32m     87\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mjaxlib\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mxla_client\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mxla_client\u001B[39;00m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: This version of jaxlib was built using AVX instructions, which your CPU and/or operating system do not support. This error is frequently encountered on macOS when running an x86 Python installation on ARM hardware. In this case, try installing an ARM build of Python. Otherwise, you may be able work around this issue by building jaxlib from source."
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "logs = defaultdict(list)\n",
    "pbar = tqdm(total=total_frames)\n",
    "eval_str = \"\"\n",
    "\n",
    "# We iterate over the collector until it reaches the total number of frames it was\n",
    "# designed to collect:\n",
    "for i, tensordict_data in enumerate(collector):\n",
    "    # we now have a batch of data to work with. Let's learn something from it.\n",
    "    for _ in range(num_epochs):\n",
    "        # We'll need an \"advantage\" signal to make PPO work.\n",
    "        # We re-compute it at each epoch as its value depends on the value\n",
    "        # network which is updated in the inner loop.\n",
    "        advantage_module(tensordict_data)\n",
    "        data_view = tensordict_data.reshape(-1)\n",
    "        replay_buffer.extend(data_view.cpu())\n",
    "        for _ in range(frames_per_batch // sub_batch_size):\n",
    "            subdata = replay_buffer.sample(sub_batch_size)\n",
    "            loss_vals = loss_module(subdata.to(device))\n",
    "            loss_value = (\n",
    "                loss_vals[\"loss_objective\"]\n",
    "                + loss_vals[\"loss_critic\"]\n",
    "                + loss_vals[\"loss_entropy\"]\n",
    "            )\n",
    "\n",
    "            # Optimization: backward, grad clipping and optimization step\n",
    "            loss_value.backward()\n",
    "            # this is not strictly mandatory but it's good practice to keep\n",
    "            # your gradient norm bounded\n",
    "            torch.nn.utils.clip_grad_norm_(loss_module.parameters(), max_grad_norm)\n",
    "            optim.step()\n",
    "            optim.zero_grad()\n",
    "\n",
    "    logs[\"reward\"].append(tensordict_data[\"next\", \"reward\"].mean().item())\n",
    "    pbar.update(tensordict_data.numel())\n",
    "    cum_reward_str = (\n",
    "        f\"average reward={logs['reward'][-1]: 4.4f} (init={logs['reward'][0]: 4.4f})\"\n",
    "    )\n",
    "    logs[\"step_count\"].append(tensordict_data[\"step_count\"].max().item())\n",
    "    stepcount_str = f\"step count (max): {logs['step_count'][-1]}\"\n",
    "    logs[\"lr\"].append(optim.param_groups[0][\"lr\"])\n",
    "    lr_str = f\"lr policy: {logs['lr'][-1]: 4.4f}\"\n",
    "    if i % 10 == 0:\n",
    "        # We evaluate the policy once every 10 batches of data.\n",
    "        # Evaluation is rather simple: execute the policy without exploration\n",
    "        # (take the expected value of the action distribution) for a given\n",
    "        # number of steps (1000, which is our ``env`` horizon).\n",
    "        # The ``rollout`` method of the ``env`` can take a policy as argument:\n",
    "        # it will then execute this policy at each step.\n",
    "        with set_exploration_type(ExplorationType.DETERMINISTIC), torch.no_grad():\n",
    "            # execute a rollout with the trained policy\n",
    "            eval_rollout = env.rollout(1000, policy_module)\n",
    "            logs[\"eval reward\"].append(eval_rollout[\"next\", \"reward\"].mean().item())\n",
    "            logs[\"eval reward (sum)\"].append(\n",
    "                eval_rollout[\"next\", \"reward\"].sum().item()\n",
    "            )\n",
    "            logs[\"eval step_count\"].append(eval_rollout[\"step_count\"].max().item())\n",
    "            eval_str = (\n",
    "                f\"eval cumulative reward: {logs['eval reward (sum)'][-1]: 4.4f} \"\n",
    "                f\"(init: {logs['eval reward (sum)'][0]: 4.4f}), \"\n",
    "                f\"eval step-count: {logs['eval step_count'][-1]}\"\n",
    "            )\n",
    "            del eval_rollout\n",
    "    pbar.set_description(\", \".join([eval_str, cum_reward_str, stepcount_str, lr_str]))\n",
    "\n",
    "    # We're also using a learning rate scheduler. Like the gradient clipping,\n",
    "    # this is a nice-to-have but nothing necessary for PPO to work.\n",
    "    scheduler.step()"
   ],
   "id": "fcd73ded2632c2af"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(logs[\"reward\"])\n",
    "plt.title(\"training rewards (average)\")\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(logs[\"step_count\"])\n",
    "plt.title(\"Max step count (training)\")\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(logs[\"eval reward (sum)\"])\n",
    "plt.title(\"Return (test)\")\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.plot(logs[\"eval step_count\"])\n",
    "plt.title(\"Max step count (test)\")\n",
    "plt.show()"
   ],
   "id": "2d277d1a475a0be0"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
